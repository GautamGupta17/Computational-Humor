{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa \n",
    "import math\n",
    "from glob import glob\n",
    "import argparse\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawClip3(object):\n",
    "    \"\"\"Loads audio clips from disk, applies a rolling window, and\n",
    "    extracts features from each sample.\"\"\"\n",
    "    featureFuncs = ['tonnetz', 'spectral_rolloff', 'spectral_contrast',\n",
    "                    'spectral_bandwidth', 'spectral_flatness', 'mfcc',\n",
    "                    'chroma_cqt', 'chroma_cens', 'melspectrogram']\n",
    "\n",
    "    def __init__(self, sourcefile, Y_class=None):\n",
    "        self.y, self.sr = sf.read(sourcefile)\n",
    "        self.laughs = None\n",
    "        self.Y_class = Y_class\n",
    "\n",
    "    def resample(self, rate, channel):\n",
    "        return librosa.resample(self.y.T[channel], self.sr, rate)\n",
    "\n",
    "    def amp(self, rate=22050, n_fft=2048, channel=0):\n",
    "        D = librosa.amplitude_to_db(librosa.magphase(librosa.stft(\n",
    "            self.resample(rate, channel), n_fft=n_fft))[0], ref=np.max)\n",
    "        return D\n",
    "\n",
    "    def _extract_feature(self, func):\n",
    "        method = getattr(librosa.feature, func)\n",
    "\n",
    "        # Construct params for each 'class' of features\n",
    "        params = {'y': self.raw}\n",
    "        if 'mfcc' in func:\n",
    "            params['sr'] = self.sr\n",
    "            params['n_mfcc'] = 128\n",
    "        if 'chroma' in func:\n",
    "            params['sr'] = self.sr\n",
    "\n",
    "        feature = method(**params)\n",
    "\n",
    "        return feature\n",
    "\n",
    "    def _split_features_into_windows(self, data, duration):\n",
    "        # Apply a moving window\n",
    "        windows = []\n",
    "\n",
    "        # Pad the rightmost edge by repeating frames, simplifies stretching\n",
    "        # the model predictions to the original audio later on.\n",
    "        data = np.pad(data, [[0, duration], [0, 0]], mode='edge')\n",
    "        for i in range(data.shape[0] - duration):\n",
    "            windows.append(data[i:i + duration])\n",
    "\n",
    "        return np.array(windows)\n",
    "\n",
    "    def build_features(self, duration=30, milSamplesPerChunk=10):\n",
    "        # Extract features, one chunk at a time (to reduce memory required)\n",
    "        # Tip: about 65 million samples for a normal-length episode\n",
    "        # 10 million samples results in around 1.5GB to 2GB memory use\n",
    "        features = []\n",
    "\n",
    "        chunkLen = milSamplesPerChunk * 1000000\n",
    "        numChunks = math.ceil(self.y.shape[0] / chunkLen)\n",
    "\n",
    "        for i in range(numChunks):\n",
    "            # Set raw to the current chunk, for _extract_feature\n",
    "            self.raw = self.y.T[0][i * chunkLen:(i + 1) * chunkLen]\n",
    "\n",
    "            # For this chunk, run all of our feature extraction functions\n",
    "            # Each returned array is in the shape (features, steps)\n",
    "            # Use concatenate to combine (allfeatures, steps)\n",
    "            chunkFeatures = np.concatenate(\n",
    "                list(\n",
    "                    map(self._extract_feature, self.featureFuncs)\n",
    "                )\n",
    "            )\n",
    "            features.append(chunkFeatures)\n",
    "\n",
    "        # Transform to be consistent with our LSTM expected input\n",
    "        features = np.concatenate(features, axis=1).T\n",
    "        # Combine our chunks along the time-step axis.\n",
    "        features = self._split_features_into_windows(features, duration)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaughRemover(object):\n",
    "    \"\"\"Contains the logic to apply predictions as audio transformations\"\"\"\n",
    "    def __init__(self, kerasModel=None, kerasModelFile=None):\n",
    "        import keras\n",
    "        assert kerasModel or kerasModelFile\n",
    "        if kerasModel:\n",
    "            self.model = kerasModel\n",
    "        elif kerasModelFile:\n",
    "            self.model = keras.models.load_model(filepath=kerasModelFile)\n",
    "\n",
    "    def remove_laughs(self, infile, outfile):\n",
    "        \"\"\"Remove laughs from a single sound file\"\"\"\n",
    "        rc = RawClip3(infile)\n",
    "        rc.laughs = self.model.predict(rc.build_features())\n",
    "        self._apply_laughs_array(rc.y, rc.sr, outfile, rc.laughs[:, 0])\n",
    "        return rc\n",
    "\n",
    "    def batch_remove_laughs(self, indir : str, outdir: str, batch_size: int=32):\n",
    "        \"\"\"Remove laughs from all files in a directory\"\"\"\n",
    "        # If indir == outdir, processes files in-place \n",
    "        batch_of_features = []\n",
    "        for filename in os.listdir(indir):\n",
    "            rc = RawClip3(os.path.join(indir, filename))\n",
    "            features = rc.build_features()\n",
    "            # Need to add some form of padding to each file so that it can be batched for keras.\n",
    "            # Then need to unpad so that original file duration is restored.\n",
    "            # Right now, it just loads the model once, and runs all the files through it one-by-one. \n",
    "            rc.laughs = self.model.predict(features)\n",
    "            self._apply_laughs_array(rc.y, rc.sr, os.path.join(outdir, filename), rc.laughs[:, 1])\n",
    "\n",
    "    def _apply_laughs_array(self, y, sr, outfile, laughs):\n",
    "        y.T[0] = self._apply_frames_to_samples(frames=laughs, samples=y.T[0])\n",
    "\n",
    "        y.T[1] = self._apply_frames_to_samples(frames=laughs, samples=y.T[1])\n",
    "\n",
    "        sf.write(outfile, y, sr) \n",
    "\n",
    "    def _apply_frames_to_samples(self, frames, samples, exp=1, period=15):\n",
    "        # Apply a rolling average to smooth the laugh/notlaugh sections\n",
    "        frames = np.convolve(frames, np.ones((period,)) / period, mode='same')\n",
    "        # Each frame = default 512 samples, so expand over that period\n",
    "        frames = np.repeat(frames, librosa.core.frames_to_samples(1))\n",
    "        # Trim excess padding off the rightmost end\n",
    "        frames = frames[:len(samples)]\n",
    "        # Finally, apply audio volume change\n",
    "        return samples * (frames ** exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_mute_laughs(sourceFile, outFile, model):\n",
    "    params = {}\n",
    "    if type(model) == str:\n",
    "        params['kerasModelFile'] = model\n",
    "    else:\n",
    "        params['kerasModel'] = model\n",
    "\n",
    "    laughr = LaughRemover(**params)\n",
    "\n",
    "    arr=laughr.remove_laughs(sourceFile, outFile)\n",
    "\n",
    "    return arr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_1 (Batc  (None, 30, 296)          1184      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 30, 64)            92416     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 30, 64)            33024     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 159,778\n",
      "Trainable params: 159,186\n",
      "Non-trainable params: 592\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "Model = load_model('model.h5')\n",
    "\n",
    "print(Model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_params(sourceFile, outFile, model):\n",
    "    params = {}\n",
    "    if type(model) == str:\n",
    "        params['kerasModelFile'] = model\n",
    "    else:\n",
    "        params['kerasModel'] = model\n",
    "\n",
    "    print(params.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('kerasModel', <keras.engine.sequential.Sequential object at 0x00000276F3B925F0>)])\n"
     ]
    }
   ],
   "source": [
    "print_params(sourceFile='mn.wav',\n",
    "                outFile='output.wav',\n",
    "                model=Model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "arr = do_mute_laughs(sourceFile='02-ff.wav',\n",
    "                outFile='output.wav',\n",
    "                model=Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RawClip3 at 0x2768f88aad0>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "985/985 [==============================] - 23s 23ms/step\n"
     ]
    }
   ],
   "source": [
    "arr = do_mute_laughs(sourceFile='mn.wav',\n",
    "                outFile='mnnn.wav',\n",
    "                model=Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1940400\n"
     ]
    }
   ],
   "source": [
    "y, sr = librosa.load('compiled.wav')\n",
    "\n",
    "def avg_amplitude(y, sr):\n",
    "    second = []\n",
    "    for s in range(0,len(y),sr):\n",
    "        second.append( y[s:s+sr].mean() )\n",
    "    print(s)\n",
    "\n",
    "avg_amplitude(y,sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized energy of file 1: 26136.44\n",
      "Normalized energy of file 2: 17356.37\n",
      "Normalized amplitude of file 1: 0.20\n",
      "Normalized amplitude of file 2: 0.17\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "import scipy\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "# Define the laughter frequency range (100 Hz to 500 Hz)\n",
    "laughter_min_freq = 100\n",
    "laughter_max_freq = 500\n",
    "\n",
    "# Define the window and hop sizes for the STFT\n",
    "window_size = 2048\n",
    "hop_size = 512\n",
    "\n",
    "# Load the first audio file\n",
    "audio1, sr1 = librosa.load('12-ff.wav', sr=None)\n",
    "\n",
    "# Apply a bandpass filter to isolate the laughter frequency range\n",
    "b, a = scipy.signal.butter(2, [laughter_min_freq/(sr1/2), laughter_max_freq/(sr1/2)], btype='band')\n",
    "audio1_filtered = scipy.signal.filtfilt(b, a, audio1)\n",
    "\n",
    "# Calculate the STFT of the filtered audio signal\n",
    "stft1 = librosa.stft(audio1_filtered, n_fft=window_size, hop_length=hop_size)\n",
    "\n",
    "# Calculate the spectral energy within the laughter frequency range at each time frame of the STFT\n",
    "laughter_energy1 = np.sum(np.abs(stft1[(laughter_min_freq*window_size//sr1):(laughter_max_freq*window_size//sr1), :])**2, axis=0)\n",
    "\n",
    "# Compute the total energy and peak amplitude of the laughter waveform\n",
    "total_energy1 = np.sum(laughter_energy1)\n",
    "peak_amplitude1 = np.max(audio1_filtered)\n",
    "\n",
    "# Normalize the total energy and peak amplitude values by the duration of the audio file\n",
    "duration1 = len(audio1)/sr1\n",
    "normalized_energy1 = total_energy1/duration1\n",
    "normalized_amplitude1 = peak_amplitude1\n",
    "\n",
    "# Load the second audio file\n",
    "audio2, sr2 = librosa.load('08-ff.wav', sr=None)\n",
    "\n",
    "# Apply the same processing to the second audio file\n",
    "audio2_filtered = scipy.signal.filtfilt(b, a, audio2)\n",
    "stft2 = librosa.stft(audio2_filtered, n_fft=window_size, hop_length=hop_size)\n",
    "laughter_energy2 = np.sum(np.abs(stft2[(laughter_min_freq*window_size//sr2):(laughter_max_freq*window_size//sr2), :])**2, axis=0)\n",
    "total_energy2 = np.sum(laughter_energy2)\n",
    "peak_amplitude2 = np.max(audio2_filtered)\n",
    "duration2 = len(audio2)/sr2\n",
    "normalized_energy2 = total_energy2/duration2\n",
    "normalized_amplitude2 = peak_amplitude2\n",
    "\n",
    "# Compare the normalized intensity and frequency values between the two audio files\n",
    "print(f\"Normalized energy of file 1: {normalized_energy1:.2f}\")\n",
    "print(f\"Normalized energy of file 2: {normalized_energy2:.2f}\")\n",
    "print(f\"Normalized amplitude of file 1: {normalized_amplitude1:.2f}\")\n",
    "print(f\"Normalized amplitude of file 2: {normalized_amplitude2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def laughter_score(audio_file):\n",
    "    # Load audio file and extract features\n",
    "    y, sr = librosa.load(audio_file)\n",
    "    laughter_segments = librosa.effects.split(y, top_db=20)\n",
    "    laughter_duration = librosa.get_duration(y)\n",
    "    num_laugh_segments = len(laughter_segments)\n",
    "    laugh_variability = np.std(librosa.feature.rms(y=librosa.effects.trim(y)[0]))\n",
    "    laughs_per_minute = num_laugh_segments / (laughter_duration / 60)\n",
    "    max_loudness = np.max(y)\n",
    "    energy = np.sum(y ** 2)\n",
    "\n",
    "    # Assign weights to each feature\n",
    "    duration_weight = 0.2\n",
    "    num_laugh_weight = 0.1\n",
    "    variability_weight = 0.2\n",
    "    laughs_per_min_weight = 0.1\n",
    "    max_loudness_weight = 0.2\n",
    "    energy_weight = 0.2\n",
    "\n",
    "    # Normalize each feature to a score between 0 and 10\n",
    "    duration_score = (laughter_duration / 20) * duration_weight\n",
    "    num_laugh_score = (num_laugh_segments / 5) * num_laugh_weight\n",
    "    variability_score = (laugh_variability / 0.15) * variability_weight\n",
    "    laughs_per_min_score = (laughs_per_minute / 5) * laughs_per_min_weight\n",
    "    max_loudness_score = (max_loudness / 2) * max_loudness_weight\n",
    "    energy_score = (energy / 2) * energy_weight\n",
    "\n",
    "    # Combine normalized scores into final score\n",
    "    final_score = (duration_score + num_laugh_score + variability_score + laughs_per_min_score + max_loudness_score + energy_score) * 10 / (duration_weight + num_laugh_weight + variability_weight + laughs_per_min_weight + max_loudness_weight + energy_weight)\n",
    "\n",
    "    return final_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307.78302942180903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naren\\AppData\\Local\\Temp\\ipykernel_14820\\2560824405.py:8: FutureWarning: Pass y=[ 2.0866487e-03  3.0331282e-04 -3.5524319e-03 ... -2.1439803e-09\n",
      "  1.0093950e-09  0.0000000e+00] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  laughter_duration = librosa.get_duration(y)\n"
     ]
    }
   ],
   "source": [
    "print(laughter_score('output.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5129751925799706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naren\\AppData\\Local\\Temp\\ipykernel_15268\\2129986175.py:8: FutureWarning: Pass y=[0.02873103 0.0580011  0.05970972 ... 0.09232318 0.08300322 0.        ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  onset_env = librosa.onset.onset_strength(audio, sr=sr)\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Load the audio file\n",
    "audio, sr = librosa.load('compiled.wav')\n",
    "\n",
    "# Detect onsets in the laughter segment\n",
    "onset_env = librosa.onset.onset_strength(audio, sr=sr)\n",
    "onset_frames = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr)\n",
    "\n",
    "# Extract the laughter segments\n",
    "segments = []\n",
    "for i in range(len(onset_frames)):\n",
    "    start = onset_frames[i]\n",
    "    if i == len(onset_frames) - 1:\n",
    "        end = audio.size\n",
    "    else:\n",
    "        end = onset_frames[i+1]\n",
    "    segment = audio[start:end]\n",
    "    segments.append(segment)\n",
    "\n",
    "# Compute the intensity of each laugh segment\n",
    "intensities = []\n",
    "for segment in segments:\n",
    "    energy = np.sum(segment**2)\n",
    "    rms_energy = np.sqrt(energy / segment.size)\n",
    "    intensity = 10 * np.log10(energy / segment.size)\n",
    "    intensities.append(intensity)\n",
    "\n",
    "# Normalize the intensities to a range of 0 to 1\n",
    "max_intensity = np.max(intensities)\n",
    "norm_intensities = intensities / max_intensity\n",
    "\n",
    "# Adjust the length of the weighting array to match the number of laughs\n",
    "w = np.ones(len(norm_intensities))\n",
    "w /= np.sum(w)\n",
    "\n",
    "# Compute the weighted average of the normalized intensities\n",
    "score = np.dot(w, norm_intensities)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intensity of laughter: 8.811465228085329\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "\n",
    "# Define a function to calculate the intensity of laughter using RMS value and duration\n",
    "def calculate_intensity(audio_file):\n",
    "    y, sr = librosa.load(audio_file, sr=None)  # Load the audio file\n",
    "    y, _ = librosa.effects.trim(y)  # Remove silent portions of the signal\n",
    "    \n",
    "    # Calculate the RMS value of the signal\n",
    "    rms = np.sqrt(np.mean(y ** 2))\n",
    "    \n",
    "    # Calculate the duration of the laughter in seconds\n",
    "    duration = librosa.get_duration(y=y, sr=sr)\n",
    "    \n",
    "    # Calculate the intensity of the laughter by normalizing the RMS value with the duration\n",
    "    intensity = rms / duration\n",
    "    \n",
    "    return intensity*10000\n",
    "\n",
    "# Example usage\n",
    "intensity = calculate_intensity('compiled.wav')\n",
    "print(f'Intensity of laughter: {intensity}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25f5a0c83a496fdcf8d064ab469e301a06d19a0e1bbb7f82387ec11f7ef14d37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
