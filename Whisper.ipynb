{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt6Z5T5gUz4l",
        "outputId": "ef7b09c0-f6c4-4361-d894-a1df72d0a822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jan 17 16:17:21 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yL41-4zfVB1p",
        "outputId": "3408c0cc-2e78-4a14-d025-2db75ee38b5b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-tv1_mdqy\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-tv1_mdqy\n",
            "  Resolved https://github.com/openai/whisper.git to commit 0f39c89d9212e4d0c64b915cf7ba3c1f0b59fecc\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from whisper==1.0) (1.21.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from whisper==1.0) (1.13.0+cu116)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from whisper==1.0) (4.64.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from whisper==1.0) (9.0.0)\n",
            "Collecting transformers>=4.19.0\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpeg-python==0.2.0\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from ffmpeg-python==0.2.0->whisper==1.0) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->whisper==1.0) (3.9.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->whisper==1.0) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->whisper==1.0) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->whisper==1.0) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->whisper==1.0) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->whisper==1.0) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers>=4.19.0->whisper==1.0) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->whisper==1.0) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->whisper==1.0) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->whisper==1.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->whisper==1.0) (1.24.3)\n",
            "Building wheels for collected packages: whisper\n",
            "  Building wheel for whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for whisper: filename=whisper-1.0-py3-none-any.whl size=1175342 sha256=685a7e7410f922417469e62155c85f9df31dce3e5c78f5a0073f721e058fc0c6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xrh2plal/wheels/a7/70/18/b7693c07b1d18b3dafb328f5d0496aa0d41a9c09ef332fd8e6\n",
            "Successfully built whisper\n",
            "Installing collected packages: tokenizers, ffmpeg-python, huggingface-hub, transformers, whisper\n",
            "Successfully installed ffmpeg-python-0.2.0 huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1 whisper-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the libraries \n",
        "import whisper\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Initialize the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the model \n",
        "whisper_model = whisper.load_model(\"large\", device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8l19zcgU-Gu",
        "outputId": "b61ea881-39d7-4af6-8025-4d06e842442a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████| 2.87G/2.87G [00:24<00:00, 125MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the module\n",
        "!pip install pytube\n",
        "\n",
        "# Import the module\n",
        "from pytube import YouTube"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n5M5ThlU-J4",
        "outputId": "76fa1e07-3137-40e2-fb34-65211a50ebf6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytube\n",
            "  Downloading pytube-12.1.2-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-12.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def video_to_audio(video_URL, destination, final_filename):\n",
        "\n",
        "  # Get the video\n",
        "  video = YouTube(video_URL)\n",
        "\n",
        "  # Convert video to Audio\n",
        "  audio = video.streams.filter(only_audio=True).first()\n",
        "\n",
        "  # Save to destination\n",
        "  output = audio.download(output_path = destination)\n",
        "\n",
        "  _, ext = os.path.splitext(output)\n",
        "  new_file = final_filename + '.mp3'\n",
        "\n",
        "  # Change the name of the file\n",
        "  os.rename(output, new_file)"
      ],
      "metadata": {
        "id": "H8XR1CSbU-Mi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://www.youtube.com/watch?v=viSAw3Zyr2Y\"\n",
        "final_filename = \"comic\"\n",
        "destination = \".\"\n",
        "video_to_audio(URL, destination, final_filename)\n",
        "\n",
        "# Run the test\n",
        "audio_file = \"comic.mp3\"\n",
        "hindi_to_english = whisper_model.transcribe(audio_file)\n",
        "\n",
        "# Show the result\n",
        "print(hindi_to_english[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNT5WRgthMzn",
        "outputId": "c22d7c52-ddc6-4741-f32f-91a64455525f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " तब आपको बात करने के लिए खुश देखें तो जब से ये वैकसिंग वाली वीडियो आई है भाई साहब या मेरे इंस्ट्राग्राम पे इतने गंदे गंदे फोटोगे न मुझे हर उस शक्स का फोटो है मेरे पाद जिसके शरीटो बाल मुझे कोई भी लॉणडा कहीं से बेज़ दे रहा है भाईया ये देखो मेरी भूहा के लोड़का आपके जैसा अरे तुम पागल है कुछ या भालू के साथ टैक कर रखा है मेरे बड़े में भालू तो लगी या इस टावे कुछ कह भी नहीं सकते किसी को स्क्रीन चॉट ले लेते लॉणडा एक तो जितनी महिला काम करती ना अधन कैप में वो इन जैसे लॉणडो ने सबकी वोड़ो भेज रखे ले तो देखी या ये मनजू आंटी ये सल्ला आंटी था चुप चाप और्टो की वोड़ो की ज़िया पिड़ जाएगा बैट सो था कौन है सल्ला आंटी था कौन है एक चोट यह होता रहे है अच्छा हाद उठा के बता भी ना है कुछ यह बड़ा लेंड यादनी है तो क्या करता है तुझे क्या लग रहे है मैं चॉकलेट ने था तुझे था कुछ क्या करता है मेरे भाई का क्या काम करता है सल्ला आंटी के दर्बाल पकड़ी चलता है कारे काम क्या कर रहे है काम तो ज़मी करते हैं भाई क्या दोही है जो सही मेर अर्बन क्लैप में काम करता है मेरा एड कहा है अपनी कमपनी से एक बात बोल कि अराम करो दो महीने मेरी वीडियो को तुमने अपना एड चलाया मुझे क्यों नहीं ले रहो परी बैट है न लोग बहुती प्यारा यार बैंगलार सिटी से प्यार हो गया मुझे मैंने लाइब सिटी से नहीं तुम लोगों से सिटी में रहते हो तुम होटेल भी नहीं पहुंच पाते हो गाड़ी में बेटे जीट के तो कितना देर दिखा रहा है कहा बत बैट रहे है अभी पूरा शहर लाल हो चुगा हमारा क्या करते है भाई काम तो सब करते हैं भेंचो पागल हो गये हो तुम ये कैसा जवाब है ये क्या करते हैं कामर कर प्या कुछ भी कर में ते था आप क्या काम करते हैं भी ट्रांस्पोरत का काम करते हो बैंगालोर में तो ट्रांस्पोट का... ये बोलना Course consist ये बोलना मैं एक जगा बैटा रहता हूँ.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://www.youtube.com/watch?v=viSAw3Zyr2Y\"\n",
        "final_filename = \"comic\"\n",
        "destination = \".\"\n",
        "video_to_audio(URL, destination, final_filename)\n",
        "\n",
        "# Run the test\n",
        "audio_file = \"comic.mp3\"\n",
        "hindi_to_english = whisper_model.transcribe(audio_file, task = 'translate')\n",
        "\n",
        "# Show the result\n",
        "print(hindi_to_english[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYM3bLI7jwxS",
        "outputId": "80bdf0ae-58af-45d3-fcd8-4446758e100c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Thank you for coming. Thank you. What kind of a talk is this? I told you to speak in a Sallah-anti way. Is this a talk to be done? Shout my name, is it bad? So, since this waxing video has come, Brother, I have so many dirty photos on my Instagram. I have a photo of every person whose body and hair. Any boy is sending me from somewhere. Brother, look at this, my aunt's son is like you. Are you mad? I have been tagged with a bear. I am not a bear. I can't say anything to anyone on Instagram. Let's take a screenshot. All the women who work in the Northern Cap, They have sent me their photos. Manju aunty or Sallah aunty? I will get beaten up like a woman's photo. Who was Sallah aunty? Something is happening. Okay, raise your hand and tell me. You are from a big land, what do you do? Do you think I give you chocolate? I will beat you up. What does my brother do? He works. What does he do? Does he walk holding Sallah aunty's hair? What does he do? He works in the land. He really works in the urban club. Where is my ad? Tell your company, You have been running your ad for 2 months on my video. Why are you not hiring me? People are sitting upstairs. I am in love with Bangalore city. I mean not with the city. With you people. You live in the city. You can't even reach the hotel. You are sitting in the car. The GP is showing you how long you have to wait. He is telling you to sit. Our whole city has turned red. Whose beer is kept here? What happened? Why is it kept here? You both are telling each other. I am not your teacher. Why are you telling each other? What do you do? We work. You work. Are you mad? What kind of answer is this? What do you do? You do anything. What do you do? You work in transport. In Bangalore, you work in transport. I am sitting at one place.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://www.youtube.com/watch?v=ZOPhSSFYRIo\"\n",
        "final_filename = \"comic\"\n",
        "destination = \".\"\n",
        "video_to_audio(URL, destination, final_filename)\n",
        "\n",
        "# Run the test\n",
        "audio_file = \"comic.mp3\"\n",
        "hindi_to_english = whisper_model.transcribe(audio_file, task = 'translate')\n",
        "\n",
        "# Show the result\n",
        "print(hindi_to_english[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5PoOEZ2WCT5",
        "outputId": "6748e951-4859-401f-ec59-0aeb55442e69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " But now I have a lot of empathy for cats. I mean, cats will come this way. I mean, I keep seeing cats everywhere. What happened to it? Did it get cut or not? Show me, sis. Oh, I see. I mean, people shouldn't treat cats wrongly. Treat humans instead. In fact, one day I was getting down from my house. And there was a cat in my building. And I saw someone giving it potato fritters to eat. Potato fritters. To a carnivorous animal. I was the security guard. I said, listen, let's have a cup of tea together. And I put it on. And the news would sit and watch, sis. Who put the potato fritters? I said, madam, what happened to the food? I said, they are carnivorous. She said, I don't know what you are saying. I said, that's right. I was going to the market in the morning. I got the cat food and came back to give it to the cat. I said, take this food. So he said, what is this? I said, this is cat food. He said, how much is it? I said, for 50 rupees. He said, oh, I see. He left. A week later, the guard was waiting for me. He said, listen, come here. I am hungry, get me breakfast. Give me 30 rupees for the fritters and tea. I will go and get it. I said, if I am your financial agent, go and do it yourself. He said, you can spend 50 rupees on a cat. You can't give 30 rupees to a hungry person. I said, that's right. He said, take this 30 rupees. I said, listen, tomorrow I am going to kick the cat. Be ready. And after 6 months, I will get it chopped. Okay? So enjoy these last 6 months.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://www.youtube.com/watch?v=ZOPhSSFYRIo\"\n",
        "final_filename = \"comic\"\n",
        "destination = \".\"\n",
        "video_to_audio(URL, destination, final_filename)\n",
        "\n",
        "# Run the test\n",
        "audio_file = \"comic.mp3\"\n",
        "hindi_to_english = whisper_model.transcribe(audio_file)\n",
        "\n",
        "# Show the result\n",
        "print(hindi_to_english[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaqqNgTEWCWv",
        "outputId": "ead00b07-fe51-4647-8199-ce41b1fb6f8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " पर मेरी बिल्लों के लिए बहुत एमपथी आ गई है। बिल्लों के लिए तो आयी जाएगी, बेहंच। मतलब हर ज़्यगा मैं बिल्ले देखते रहता हूँ। कि उसका क्या हुआ है? कटे है की नहीं कटे है? दिका बेहंच। अच्छे। अच्छे। लोग मैं सत्मे लोग मैं कहा बिल्लों का लोग गलट टीट नहीं करने चाहें, इंसानों को कर दो। मैं सेक्योटी गार था, मैं कहा सुनो एक चाय भी रहते थे साथ मैं और लगाया थे निउस बैट के देखते रहा था बेहंच। आलू बुज़े कौन डाल गया? कहा रहा मैड़ं देखे खाना क्या हुआ? मैं कहा कार नी वर्स होते हैं ये. कहा करें पता न ही क्या बोल रहा आँ? मैंकर सही बात..मैं बरुकर जा रहा हुआ था, मैं लेकार क्हाना लेया और वापट आके बिल्ले को खाना देना लगी आनू मैं करें एले खाना गया। तो कहता कहा, हई है क्या है? मैं कहा वर से बिल्ले का माग ते तरा फ़िनांसल लगों, खुज़ जा कर ले बेंच। कहा रहा है बिल्ले पे पचाथ सरपाई कहर्च कर सकते हो। इनसान को भूख लगे तीस रुपएं दे सकते हैं। आगर सही बात है। माग कहा ये लो भीया तीस रुपएं। आगर नहीं पचाथ सरपाई कर ले बेंच।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://www.youtube.com/shorts/sEQSNbvRSQ4\"\n",
        "final_filename = \"comic\"\n",
        "destination = \".\"\n",
        "video_to_audio(URL, destination, final_filename)\n",
        "\n",
        "# Run the test\n",
        "audio_file = \"comic.mp3\"\n",
        "hindi_to_english = whisper_model.transcribe(audio_file, task = 'translate')\n",
        "\n",
        "# Show the result\n",
        "print(hindi_to_english[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBWtk2bjwevT",
        "outputId": "8bce5d7f-c292-4795-dec9-8ff80e5513f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I feel like the vagina is a lot like a university. You gotta be accepted. And much like university, easier to get in if you have a lot of money. You're like, well how'd that guy get in? He doesn't have any money. Well, he's an athlete. Yeah, very similar to university. Some are very prestigious. Like, wow, you got in there? That's impressive. Good for you. Your other friend's like, well I got in there. You're like, well the whole community's been in there. And you only got in because your dad got in.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "loLr21vGweya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "50-DJAwVWCZV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}